# -*- coding: utf-8 -*-
"""Copy of project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13femrJgKlwLgvsXW2gfs_7QjfehllhPF
"""

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd

training_data = pd.read_csv('/content/drive/MyDrive/split_data/train_data.csv')

# Count the occurrences of each sentiment label
sentiment_counts = training_data['sentiment'].value_counts()

# Print the counts for each sentiment
print(sentiment_counts)

#sad+worry+hate+anger+delempty=3937-827=3110
#neutal+relief+boredom+surprise= 3179
#+love+fun+happiness+ delenthus=3000

# deleting the empty and enthusiasm emotion for balancing
# #  Remove rows with "empty" sentiment
# training_data = training_data[training_data['sentiment'] != 'boredom']
#  # Remove rows with "empty" sentiment
# training_data = training_data[training_data['sentiment'] != 'anger']

#sad+worry+hate+anger+delempty=3937-827=3110 - negative
#neutral+relief+boredom+surprise= 3179 - neutral
#love+fun+happiness=3000 - positive

# #dividing the dataset into training-60% rest-40% (validate-20% and test-20%)

# from sklearn.model_selection import train_test_split


# # Split the data into training (60%), validation (20%), and test (20%) sets

# #split the data such that training_data=60% and remaining_data=40% by using train_test_split function of sklearn library
# training_data, remaining_40_data = train_test_split(data, test_size=0.4, random_state=42)

# #split the  remaining_data=40% such that 50% of rem_data is for validation and other
# #50 of rem_data is for testing split by train_test_split function of sklearn library

# validation_data, test_data = train_test_split(remaining_40_data, test_size=0.5, random_state=42)

# # Print the number of rows in each split
# print("Training set rows:", len(training_data))
# print("Validation set rows:", len(validation_data))
# print("Test set rows:", len(test_data))

#5573+1858+1858=
#Training set rows: 5573
#Validation set rows: 1858
#Test set rows: 1858

# Save training data to a CSV file
# training_data.to_csv('train_data.csv', index=False)

# # Save testing data to a CSV file
# test_data.to_csv('test_data.csv', index=False)

# #save validating data to csv files
# validation_data.to_csv('validation_data.csv' , index = False)

# from google.colab import files

# files.download('train_data.csv')
# files.download('test_data.csv')
# files.download('validation_data.csv')

# function to label the sentimes
sentiment_mapping={
    "love":"positive",
    "fun" : "positive",
    "happiness":"positive",
    "sadness":"negative",
    "worry":"negative",
    "hate":"negative",
    "relief":"neutral",
    "surprise":"neutral",
    "neutral":"neutral"
}

# Create a new sentiment label column based on mapping
training_data["label"] = training_data["sentiment"].map(sentiment_mapping)

training_data.columns

training_data=training_data.reset_index()

training_data

# Count the occurrences of each sentiment label
sentiment_counts = training_data['label'].value_counts()

# Print the counts for each sentiment
print(sentiment_counts)

#applying lowercasing for all column
training_data['sentiment']=training_data['sentiment'].str.lower()
training_data['content']=training_data['content'].str.lower()
training_data['label']=training_data['label'].str.lower()

training_data=training_data.drop(columns="index")

import re
# 2nd thing in preprocessing: removing all urls from the dataset
def remove_urls(text):
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    cleaned_text = re.sub(url_pattern, '', text)
    return cleaned_text

training_data['content'] = training_data['content'].apply(remove_urls)

training_data.head()

training_data.isnull().sum()

#3rd steps in preprocessing : removing all @mentions from the content
# Define a function to remove words starting with "@"
def remove_at_mentions(text):
    return re.sub(r'@\w+\s*', '', text)

# Apply the function to the "content" column
training_data['content'] = training_data['content'].apply(remove_at_mentions)

training_data.head()

#4th steps in preprocessing : removing all punctuations !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~  from the content

import string
string.punctuation

#method for removing punctuation

def remove_punctuation(data):
    # Create a translation table to remove punctuation
    return data.translate(str.maketrans('', '', string.punctuation))

#applying punctuation removal from the content
training_data['content']=training_data['content'].apply(remove_punctuation)

#5th steps in preprocessing : removing all stopwords like (i,am,the,is,for etc)  from the content

import nltk
from nltk.corpus import stopwords

# Download the list of NLTK stopwords
nltk.download('stopwords')
# function to remove stopwords from data and return the clean data
def remove_stopwords(text):
    new_text = []

    for word in text.split():
        if word.lower() not in stopwords.words('english'):
            new_text.append(word)

    return " ".join(new_text)

#applying stopwords on our dataset
training_data['content']=training_data['content'].apply(remove_stopwords)

training_data.head()

#6th steps in preprocessing : expand contraction like (i'm / im= i am ,couldve = could have,dont=do not etc)  from the content
!pip install contractions
from contractions import fix
# Define a function to expand contractions
def expand_contractions(text):
  return fix(text)
# apply contraction for content column
training_data['content'] = training_data['content'].apply(expand_contractions)

print(training_data["content"])

training_data['content'].dtype

training_data['content'].isnull().sum()

training_data.head(100)

training_data['content'][557]

# import pandas as pd
import spacy

# Load the English language model (you may need to download it first)
nlp = spacy.load("en_core_web_sm")

# Define a function for sentence segmentation
def segment_text(text):
    doc = nlp(text)
    segmented_text = [sent.text for sent in doc.sents]
    return segmented_text

# Apply sentence segmentation to the "content" column
training_data["content"] = training_data["content"].apply(segment_text)

# training_data.to_csv('segmented_data.csv', index=False)

training_data["content"].dtype

# training_data['content'].isna().sum()

# # import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('wordnet')

# Function to convert POS tag to WordNet POS tag
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # default to noun


# Lemmatization function
def lemmatize_sentence(sentence):
    if isinstance(sentence, str):
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(sentence)
        tagged_tokens = nltk.pos_tag(tokens)  # Get the POS tags
        lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]
        return ' '.join(lemmatized_tokens)
    else:
        return sentence  # Return the input if it's not a string


# Tokenize and lemmatize the 'content' column
training_data['content'] = training_data['content'].apply(lemmatize_sentence)

# Now the 'content' column contains lemmatized text
print(training_data)

training_data.head(100)

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize , sent_tokenize

def tokenize_sentence(text):
    return sent_tokenize(text)

def tokenize_word(text):
    return word_tokenize(text)

segmented_data=pd.read_csv('/content/drive/MyDrive/split_data/segmented_data.csv')

segmented_data

# apply sentence tokenization for content column
segmented_data["sentence_tokenize"] = segmented_data["content"].apply(tokenize_sentence)
# apply word tokenization for content column
def word_tokenization(sentence_tokenize):
  for sentence in sentence_tokenize:
     token = word_tokenize(sentence)

     filtered_tokens = [token for token in token if token not in ['[', ']', "'", ',']]
     return filtered_tokens
    #  return token

segmented_data

segmented_data['word_tokenize'] =segmented_data['sentence_tokenize'].apply(word_tokenization)

segmented_data.head(100)

segmented_data['sentence_tokenize'][100]

segmented_data['word_tokenize'][1]

# Function to remove single quotes from the first word in each list
def remove_single_quote(word_list):
    return [word.strip("'") if word.startswith("'") else word for word in word_list]

# Apply the function to the 'content' column
segmented_data['correct_word_tokenize'] = segmented_data['word_tokenize'].apply(remove_single_quote)

segmented_data

segmented_data=segmented_data.drop(columns="word_tokenize")

# Find the indices of rows with empty lists in the 'content' column
empty_rows_indices = segmented_data[segmented_data['correct_word_tokenize'].apply(len) == 0].index
empty_rows_indices

# Indices of rows to drop (replace with your specific indices)
rows_to_drop =[224, 1785, 1792, 2057, 2421, 2598, 3689, 3920, 4072, 5008, 5138,
            5383]

# Drop the specified rows
segmented_data = segmented_data.drop(rows_to_drop)

# filtered_segmented_data.to_csv('filtered_segmented_data.csv',index=False)

# segmented_data.to_csv('checking_data.csv', index=False)

segmented_data.isnull().any()

import gensim
from gensim.models import Word2Vec
# Extract the tokenized sentences from the 'word_tokenize' column
sentences= segmented_data['correct_word_tokenize'].tolist()

# Create and train the Word2Vec model
vector_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0)

vector_model.save("word2vec.model")

model = Word2Vec.load("word2vec.model")

# vector = model.wv['good']  # Replace 'good' with the word you want to get the vector

#finding vocab_size and nth dim word embedding for each word ie size=9453 and dim=100
model.wv.vectors.shape

import numpy as np
# Define a function to calculate sentence vectors by averaging word vectors
def get_sentence_vector(sentence, model):
    sentence_vector = np.zeros(model.vector_size)  # Initialize a vector of zeros
    num_valid_words = 0  # Initialize a count of valid words

    for word in sentence:
        if word in model.wv:
            sentence_vector += model.wv[word]
            num_valid_words += 1

    if num_valid_words > 0:
        sentence_vector /= num_valid_words  # Average the vectors
        return sentence_vector
    else:
        return None  # Return None for sentences with no valid words

# Calculate sentence vectors for all sentences
sentence_vectors = [get_sentence_vector(sentence, model) for sentence in sentences]

# 'sentence_vectors' now contains the Word2Vec vectors for each sentence

model.wv.get_normed_vectors().shape

sentence_vectors[5300]

model.wv.vectors.shape
segmented_data

segmented_data['correct_word_tokenize'][2]

segmented_data['correct_word_tokenize'][0]

# Get word vectors for each word list or sentence
word_vectors_list = []

for word_list in sentences:
    word_vectors = [model.wv[word] for word in word_list]
    word_vectors_list.append(word_vectors)

# word_vectors_list now contains word vectors for each word list or sentence

segmented_data['correct_word_tokenize'][2]

word_vectors_list[2]

segmented_data['correct_word_tokenize'][0]

total_count = len(word_vectors_list)

total_count

sentence_vectors

# Determine the maximum sequence length
max_sequence_length = max(segmented_data['correct_word_tokenize'].apply(len))

max_sequence_length

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors = pad_sequences(word_vectors_list, maxlen=30, padding='pre', dtype='float32')

padded_vectors.shape

vector = model.wv['working']

vector

from sklearn.preprocessing import LabelEncoder

# Assuming your original labels are in a column named 'label' in your DataFrame
# Replace 'label' with your actual column name

label_encoder = LabelEncoder()
segmented_data['label_encoded'] = label_encoder.fit_transform(segmented_data['label'])

# Now, df['label_encoded'] contains the numerical representation of your labels
# Map the numerical representation back to the original labels if needed
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Display the mapping
print("Label Mapping:", label_mapping)

from keras.src.utils.data_utils import validate_file
# Add Word Vector column
segmented_data['word_vector_column'] = word_vectors_list

# Add Sentence Vector column
segmented_data['sentence_vector_column'] = sentence_vectors

segmented_data

# segmented_data.to_csv('numerical_training_data_with_label.csv', index=False)

# lets start of extraction of each column from files

numerical_valid_data=pd.read_csv('/content/drive/MyDrive/split_data/tokenize_valid_data_.csv')
numerical_valid_data

numerical_testing_data=pd.read_csv('/content/drive/MyDrive/split_data/tokenize_testing_data_.csv')
numerical_testing_data

from sklearn.preprocessing import LabelEncoder

# Assuming your original labels are in a column named 'label' in your DataFrame
# Replace 'label' with your actual column name
label_encoder = LabelEncoder()
numerical_testing_data['label_encoded'] = label_encoder.fit_transform(numerical_testing_data['label'])
numerical_valid_data['label_encoded'] = label_encoder.fit_transform(numerical_valid_data['label'])

numerical_valid_data

import gensim
from gensim.models import Word2Vec
# Extract the tokenized sentences from the 'word_tokenize' column
sentences_valid_data= numerical_valid_data['word_tokenize'].tolist()

sentences_test_data= numerical_testing_data['word_tokenize'].tolist()

sentences_test_data[0]

# Create and train the Word2Vec model
vector_model_valid = Word2Vec(sentences=sentences_valid_data, vector_size=100, window=5, min_count=1, sg=0)
vector_model_test = Word2Vec(sentences=sentences_test_data, vector_size=100, window=5, min_count=1, sg=0)

vector_model_valid.save("word2vec_valid.model")
vector_model_test.save("word2vec_test.model")

model_valid = Word2Vec.load("word2vec_valid.model")
model_test = Word2Vec.load("word2vec_test.model")

import numpy as np
# Define a function to calculate sentence vectors by averaging word vectors
def get_sentence_vector(sentence, model):
    sentence_vector = np.zeros(model.vector_size)  # Initialize a vector of zeros
    num_valid_words = 0  # Initialize a count of valid words

    for word in sentence:
        if word in model.wv:
            sentence_vector += model.wv[word]
            num_valid_words += 1

    if num_valid_words > 0:
        sentence_vector /= num_valid_words  # Average the vectors
        return sentence_vector
    else:
        return None  # Return None for sentences with no valid words

# Calculate sentence vectors for all sentences
sentence_vectors_valid = [get_sentence_vector(sentence, model_valid) for sentence in sentences_valid_data]

# Calculate sentence vectors for all sentences
sentence_vectors_test = [get_sentence_vector(sentence, model_test) for sentence in sentences_test_data]

sentence_vectors_valid[0]

# Get word vectors for each word list or sentence
word_vectors_list_valid = []

for word_list in sentences_valid_data:
    word_vectors = [model_valid.wv[word] for word in word_list]
    word_vectors_list_valid.append(word_vectors)

word_vectors_list_valid[0]

# Get word vectors for each word list or sentence
word_vectors_list_test = []

for word_list in sentences_test_data:
    word_vectors = [model_test.wv[word] for word in word_list]
    word_vectors_list_test.append(word_vectors)

word_vectors_list_test[0]

# numerical_valid_data=pd.read_csv('/content/drive/MyDrive/split_data/numerical_valid_data.csv')
# numerical_valid_data

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors_valid = pad_sequences(word_vectors_list_valid, maxlen=30, padding='pre', dtype='float32')

padded_vectors_valid.shape

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors_test = pad_sequences(word_vectors_list_test, maxlen=30, padding='pre', dtype='float32')

padded_vectors_test.shape

# numerical_training_data

# X_train=sentence_vectors
# X_val=sentence_vectors_valid
# X_test=sentence_vectors_test
# y_train=segmented_data['label_encoded']
# y_val=numerical_valid_data['label_encoded']
# y_test=numerical_testing_data['label_encoded']

X = [vector for vector in sentence_vectors]

# from sklearn import svm
# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import train_test_split
# from sklearn import datasets

# # Load your dataset or use a sample dataset
# # X, y = load_your_dataset()

# # Split the data into training, testing, and validation sets
# # X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
# # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# # Assuming you've already split your dataset into training, testing, and validation sets
# X_train=sentence_vectors
# X_valid=sentence_vectors_valid
# X_test=sentence_vectors_test
# y_train=segmented_data['label_encoded']
# y_valid=numerical_valid_data['label_encoded']
# y_test=numerical_testing_data['label_encoded']


# # Create an SVM model
# svm_model = svm.SVC(kernel='linear', C=1)

# # Perform cross-validation on the training set
# cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)  # You can adjust the number of folds (cv) as needed

# # Print the cross-validation scores
# print("Cross-validation scores:", cv_scores)
# print("Mean CV accuracy:", cv_scores.mean())

# # Train the model on the entire training set
# svm_model.fit(X_train, y_train)

# # Evaluate the model on the validation set
# val_accuracy = svm_model.score(X_valid, y_valid)
# print("Validation accuracy:", val_accuracy)

# # Evaluate the model on the test set
# test_accuracy = svm_model.score(X_test, y_test)
# print("Test accuracy:", test_accuracy)

X_train=X
X_valid=sentence_vectors_valid
X_test=sentence_vectors_test
y_train=segmented_data['label_encoded']
y_valid=numerical_valid_data['label_encoded']
y_test=numerical_testing_data['label_encoded']

# from sklearn import svm
# classifier=svm.SVC(kernel='linear',gamma = 'auto' ,C=2)
# classifier.fit(X_train , y_train)
# y_pred=classifier.predict(X_test)

# from sklearn.metrics import classification_report
# print(classification_report(y_test , y_pred))



from sklearn import svm
from sklearn.metrics import classification_report

X_train = X
X_valid = sentence_vectors_valid
X_test = sentence_vectors_test
y_train = segmented_data['label_encoded']
y_valid = numerical_valid_data['label_encoded']
y_test = numerical_testing_data['label_encoded']

# Using the RBF (Radial Basis Function) kernel
classifier = svm.SVC(kernel='rbf', gamma='auto', C=2)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

print(classification_report(y_test, y_pred))

