# -*- coding: utf-8 -*-
"""file_copy_for_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1csS_MFjEjbF0HL1rCcqoszDHRJdwcion
"""

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd

training_data = pd.read_csv('/content/drive/MyDrive/split_data/train_data.csv')

training_data

# Count the occurrences of each sentiment label
sentiment_counts = training_data['sentiment'].value_counts()

# Print the counts for each sentiment
print(sentiment_counts)

# data

# #creating a dataframe for storing the balanced row of each sentiment
# balanced_dataset = pd.DataFrame()
# # Loop through unique sentiment labels
# for sentiment in training_data['sentiment'].unique():
#     # Filter rows with the current sentiment
#     all_rows_containing_sentiment = training_data[training_data['sentiment'] == sentiment]

#     # Sample up to 1000 rows or all available rows, whichever is smaller
#     rows_1000_for_sentiment = all_rows_containing_sentiment.sample(n=min(1000, len(all_rows_containing_sentiment)))

#     # Append the rows to the new balanced dataset
#     balanced_dataset = balanced_dataset.append(rows_1000_for_sentiment)
#     balanced_dataset.to_csv("balanced_dataset.csv",index=False)

training_data

#sad+worry+hate+anger+delempty=3937-827=3110
#neutal+relief+boredom+surprise= 3179
#+love+fun+happiness+ delenthus=3000

# deleting the empty and enthusiasm emotion for balancing
# #  Remove rows with "empty" sentiment
# training_data = training_data[training_data['sentiment'] != 'boredom']
#  # Remove rows with "empty" sentiment
# training_data = training_data[training_data['sentiment'] != 'anger']

#Counting & printing all  the emotion available after deletion of emotion ie empty and enthusiasm

# Count the occurrences of each sentiment label
sentiment_counts = training_data['sentiment'].value_counts()

# Print the counts for each sentiment
print(sentiment_counts)

#sad+worry+hate+anger+delempty=3937-827=3110
#neutal+relief+boredom+surprise= 3179
#+love+fun+happiness+ delenthus=3000

# data

#sad+worry+hate+anger+delempty=3937-827=3110 - negative
#neutral+relief+boredom+surprise= 3179 - neutral
#love+fun+happiness=3000 - positive

# #dividing the dataset into training-60% rest-40% (validate-20% and test-20%)

# from sklearn.model_selection import train_test_split


# # Split the data into training (60%), validation (20%), and test (20%) sets

# #split the data such that training_data=60% and remaining_data=40% by using train_test_split function of sklearn library
# training_data, remaining_40_data = train_test_split(data, test_size=0.4, random_state=42)

# #split the  remaining_data=40% such that 50% of rem_data is for validation and other
# #50 of rem_data is for testing split by train_test_split function of sklearn library

# validation_data, test_data = train_test_split(remaining_40_data, test_size=0.5, random_state=42)

# # Print the number of rows in each split
# print("Training set rows:", len(training_data))
# print("Validation set rows:", len(validation_data))
# print("Test set rows:", len(test_data))

#5573+1858+1858=
#Training set rows: 5573
#Validation set rows: 1858
#Test set rows: 1858

# Save training data to a CSV file
# training_data.to_csv('train_data.csv', index=False)

# # Save testing data to a CSV file
# test_data.to_csv('test_data.csv', index=False)

# #save validating data to csv files
# validation_data.to_csv('validation_data.csv' , index = False)

# from google.colab import files

# files.download('train_data.csv')
# files.download('test_data.csv')
# files.download('validation_data.csv')

# function to label the sentimes
sentiment_mapping={
    "love":"positive",
    "fun" : "positive",
    "happiness":"positive",
    "sadness":"negative",
    "worry":"negative",
    "hate":"negative",
    "relief":"neutral",
    "surprise":"neutral",
    "neutral":"neutral"
}

# Create a new sentiment label column based on mapping
training_data["label"] = training_data["sentiment"].map(sentiment_mapping)

training_data.columns

training_data=training_data.reset_index()

training_data

# Count the occurrences of each sentiment label
sentiment_counts = training_data['label'].value_counts()

# Print the counts for each sentiment
print(sentiment_counts)

#applying lowercasing for all column
training_data['sentiment']=training_data['sentiment'].str.lower()
training_data['content']=training_data['content'].str.lower()
training_data['label']=training_data['label'].str.lower()

training_data

training_data=training_data.drop(columns="index")

training_data

import re
# 2nd thing in preprocessing: removing all urls from the dataset
def remove_urls(text):
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    cleaned_text = re.sub(url_pattern, '', text)
    return cleaned_text

training_data['content'] = training_data['content'].apply(remove_urls)

training_data.head()

training_data.isnull().sum()

#3rd steps in preprocessing : removing all @mentions from the content
# Define a function to remove words starting with "@"
def remove_at_mentions(text):
    return re.sub(r'@\w+\s*', '', text)

# Apply the function to the "content" column
training_data['content'] = training_data['content'].apply(remove_at_mentions)

training_data.head()

#4th steps in preprocessing : removing all punctuations !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~  from the content

import string
string.punctuation

#method for removing punctuation

def remove_punctuation(data):
    # Create a translation table to remove punctuation
    return data.translate(str.maketrans('', '', string.punctuation))

#applying punctuation removal from the content
training_data['content']=training_data['content'].apply(remove_punctuation)

#5th steps in preprocessing : removing all stopwords like (i,am,the,is,for etc)  from the content

import nltk
from nltk.corpus import stopwords

# Download the list of NLTK stopwords
nltk.download('stopwords')
# function to remove stopwords from data and return the clean data
def remove_stopwords(text):
    new_text = []

    for word in text.split():
        if word.lower() not in stopwords.words('english'):
            new_text.append(word)

    return " ".join(new_text)

#applying stopwords on our dataset
training_data['content']=training_data['content'].apply(remove_stopwords)

training_data.head()

#6th steps in preprocessing : expand contraction like (i'm / im= i am ,couldve = could have,dont=do not etc)  from the content
!pip install contractions
from contractions import fix
# Define a function to expand contractions
def expand_contractions(text):
  return fix(text)
# apply contraction for content column
training_data['content'] = training_data['content'].apply(expand_contractions)

print(training_data["content"])

training_data['content'].dtype

training_data['content'].isnull().sum()

training_data.head(100)

training_data['content'][557]

# import pandas as pd
import spacy

# Load the English language model (you may need to download it first)
nlp = spacy.load("en_core_web_sm")

# Define a function for sentence segmentation
def segment_text(text):
    doc = nlp(text)
    segmented_text = [sent.text for sent in doc.sents]
    return segmented_text

# Apply sentence segmentation to the "content" column
training_data["content"] = training_data["content"].apply(segment_text)

# training_data.to_csv('segmented_data.csv', index=False)

training_data["content"].dtype

# training_data['content'].isna().sum()

# # import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('wordnet')

# Function to convert POS tag to WordNet POS tag
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # default to noun


# Lemmatization function
def lemmatize_sentence(sentence):
    if isinstance(sentence, str):
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(sentence)
        tagged_tokens = nltk.pos_tag(tokens)  # Get the POS tags
        lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]
        return ' '.join(lemmatized_tokens)
    else:
        return sentence  # Return the input if it's not a string


# Tokenize and lemmatize the 'content' column
training_data['content'] = training_data['content'].apply(lemmatize_sentence)

# Now the 'content' column contains lemmatized text
print(training_data)

training_data.head(100)

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize , sent_tokenize

def tokenize_sentence(text):
    return sent_tokenize(text)

def tokenize_word(text):
    return word_tokenize(text)

training_data.head()

segmented_data=pd.read_csv('/content/drive/MyDrive/split_data/segmented_data.csv')

segmented_data

# apply sentence tokenization for content column
segmented_data["sentence_tokenize"] = segmented_data["content"].apply(tokenize_sentence)
# apply word tokenization for content column
def word_tokenization(sentence_tokenize):
  for sentence in sentence_tokenize:
     token = word_tokenize(sentence)

     filtered_tokens = [token for token in token if token not in ['[', ']', "'", ',']]
     return filtered_tokens
    #  return token

segmented_data

segmented_data['word_tokenize'] =segmented_data['sentence_tokenize'].apply(word_tokenization)

segmented_data.head(100)

segmented_data['sentence_tokenize'][100]

segmented_data['word_tokenize'][1]

# Function to remove single quotes from the first word in each list
def remove_single_quote(word_list):
    return [word.strip("'") if word.startswith("'") else word for word in word_list]

# Apply the function to the 'content' column
segmented_data['correct_word_tokenize'] = segmented_data['word_tokenize'].apply(remove_single_quote)

segmented_data

# Find the indices of rows with empty lists in the 'content' column
empty_rows_indices = segmented_data[segmented_data['correct_word_tokenize'].apply(len) == 0].index
empty_rows_indices

segmented_data['content'][224]

# Indices of rows to drop (replace with your specific indices)
rows_to_drop =[224, 1785, 1792, 2057, 2421, 2598, 3689, 3920, 4072, 5008, 5138,
            5383]

# Drop the specified rows
segmented_data = segmented_data.drop(rows_to_drop)

# filtered_segmented_data.to_csv('filtered_segmented_data.csv',index=False)

segmented_data

segmented_data.isnull().any()

import gensim
from gensim.models import Word2Vec
# Extract the tokenized sentences from the 'word_tokenize' column
sentences= segmented_data['correct_word_tokenize'].tolist()

# # Assuming 'correct_word_tokenize' column contains strings of space-separated words
# filtered_data['correct_word_tokenize'] = filtered_data['correct_word_tokenize'].apply(lambda x: x.split())

# # Extract the tokenized sentences from the 'word_tokenize' column
# sentences = filtered_data['correct_word_tokenize'].tolist()

sentences

# Create and train the Word2Vec model
vector_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0)

vector_model.save("word2vec.model")

model = Word2Vec.load("word2vec.model")

vector = model.wv['happy']  # Replace 'good' with the word you want to get the vector
vector

vector.shape



print(model)

#finding vocab_size and nth dim word embedding for each word ie size=9453 and dim=100
model.wv.vectors.shape

# import numpy as np
# # Define a function to calculate sentence vectors by averaging word vectors
# def get_sentence_vector(sentence, model):
#     sentence_vector = np.zeros(model.vector_size)  # Initialize a vector of zeros
#     num_valid_words = 0  # Initialize a count of valid words

#     for word in sentence:
#         if word in model.wv:
#             sentence_vector += model.wv[word]
#             num_valid_words += 1

#     #if num_valid_words > 0:
#        ## sentence_vector /= num_valid_words  # Average the vectors
#        # return sentence_vector
#    # else:
#         #return None  # Return None for sentences with no valid words

# # Define a function to get a list of word vectors for a sentence
# # def get_sentence_vectors(sentence, model):
#     sentence_vectors = []  # Initialize an empty list to store word vectors

#     for word in sentence:
#         if word in model.wv:
#             sentence_vectors.append(model.wv[word])

#     return sentence_vectors



#sentence_vectors

model.wv.vectors.shape
segmented_data

segmented_data['correct_word_tokenize'][2]

segmented_data['word_tokenize'][0]

# Get word vectors for each word list or sentence
word_vectors_list = []

for word_list in sentences:
    word_vectors = [model.wv[word] for word in word_list]
    word_vectors_list.append(word_vectors)

# word_vectors_list now contains word vectors for each word list or sentence

segmented_data['correct_word_tokenize'][2]

word_vectors_list[2]

segmented_data['correct_word_tokenize'][8]

total_count = len(word_vectors_list)

total_count

# Determine the maximum sequence length
max_sequence_length = max(segmented_data['correct_word_tokenize'].apply(len))

max_sequence_length

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors = pad_sequences(word_vectors_list, maxlen=30, padding='pre', dtype='float32')

padded_vectors.shape

vector = model.wv['working']

vector

from sklearn.preprocessing import LabelEncoder

# Assuming your original labels are in a column named 'label' in your DataFrame
# Replace 'label' with your actual column name

label_encoder = LabelEncoder()
segmented_data['label_encoded'] = label_encoder.fit_transform(segmented_data['label'])



segmented_data.head(10)

# Now, df['label_encoded'] contains the numerical representation of your labels
# Map the numerical representation back to the original labels if needed
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Display the mapping

print("Label Mapping:", label_mapping)

segmented_data

# segmented_data.to_csv('numerical_training_data_with_label.csv', index=False)

# lets start of extraction of each column from files

numerical_valid_data=pd.read_csv('/content/drive/MyDrive/split_data/tokenize_valid_data_.csv')
numerical_valid_data

numerical_valid_data['sentence_tokenize']

# apply sentence tokenization for content column
numerical_valid_data["sentence_tokenize"] = numerical_valid_data["content"].apply(tokenize_sentence)
# apply word tokenization for content column
def word_tokenization(sentence_tokenize):
  for sentence in sentence_tokenize:
     token = word_tokenize(sentence)

     filtered_tokens = [token for token in token if token not in ['[', ']', "'", ',']]
     return filtered_tokens
    #  return token

numerical_valid_data

numerical_testing_data=pd.read_csv('/content/drive/MyDrive/split_data/tokenize_testing_data_.csv')
numerical_testing_data



from sklearn.preprocessing import LabelEncoder

# Assuming your original labels are in a column named 'label' in your DataFrame
# Replace 'label' with your actual column name
label_encoder = LabelEncoder()
numerical_testing_data['label_encoded'] = label_encoder.fit_transform(numerical_testing_data['label'])
numerical_valid_data['label_encoded'] = label_encoder.fit_transform(numerical_valid_data['label'])



# Mapping between original labels and encoded labels
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print(label_mapping)

numerical_testing_data

segmented_data['sentence_tokenize']

numerical_testing_data['content']=numerical_testing_data['content'].tolist()

numerical_testing_data['content'][0]

import gensim
from gensim.models import Word2Vec
# Extract the tokenized sentences from the 'word_tokenize' column
sentences_valid_data= numerical_valid_data['word_tokenize'].tolist()

sentences_test_data= numerical_testing_data['word_tokenize'].tolist()

sentences[1]

sentences_valid_data

# Process the sentences and create a list of lists containing individual words
sentences_valid_data = [eval(sentence.replace('[', '').replace(']', '')) for sentence in sentences_valid_data]
sentences_valid_data

sentences_test_data

# Process the sentences and create a list of lists containing individual words
sentences_test_data = [eval(sentence.replace('[', '').replace(']', '')) for sentence in sentences_test_data]
sentences_test_data

# Create and train the Word2Vec model
vector_model_valid = Word2Vec(sentences=sentences_valid_data, vector_size=100, window=5, min_count=1, sg=0)
vector_model_test = Word2Vec(sentences=sentences_test_data, vector_size=100, window=5, min_count=1, sg=0)
vector_model_valid.save("word2vec_valid.model")
vector_model_test.save("word2vec_test.model")
model_valid = Word2Vec.load("word2vec_valid.model")
model_test = Word2Vec.load("word2vec_test.model")

vector_model_valid.save("word2vec_valid.model")
vector_model_test.save("word2vec_test.model")

model_valid = Word2Vec.load("word2vec_valid.model")
model_test = Word2Vec.load("word2vec_test.model")

import numpy as np
# Define a function to calculate sentence vectors by averaging word vectors
def get_sentence_vector(sentence, model):
    sentence_vector = np.zeros(model.vector_size)  # Initialize a vector of zeros
    num_valid_words = 0  # Initialize a count of valid words

    for word in sentence:
        if word in model.wv:
            sentence_vector += model.wv[word]
            num_valid_words += 1

    if num_valid_words > 0:
        sentence_vector /= num_valid_words  # Average the vectors
        return sentence_vector
    else:
        return None  # Return None for sentences with no valid words

# Calculate sentence vectors for all sentences
sentence_vectors_valid = [get_sentence_vector(sentence, model_valid) for sentence in sentences_valid_data]

# Calculate sentence vectors for all sentences
sentence_vectors_test = [get_sentence_vector(sentence, model_test) for sentence in sentences_test_data]

sentences_valid_data[6]

sentence_vectors_valid[6]

sentences_valid_data

sentences_valid_data

len(sentences_valid_data)



# Process the data and create a list of lists containing individual words
output_sentence_valid = []
for item in sentences_valid_data:
    if isinstance(item, tuple):
        output_sentence_valid.append(list(item))
    elif isinstance(item, str):
        output_sentence_valid.append(item.split())

output_sentence_valid

len(output_sentence_valid)

sentences_test_data

# Process the data and create a list of lists containing individual words
output_sentence_test = []

for item in sentences_test_data:
    if isinstance(item, tuple):
        output_sentence_test.append(list(item))
    elif isinstance(item, str):
        output_sentence_test.append(item.split())

print(output_sentence_test)

len(output_sentence_test)

output_sentence_test

# Create and train the Word2Vec model
vector_model_valid = Word2Vec(sentences=output_sentence_valid, vector_size=100, window=5, min_count=1, sg=0)
vector_model_test = Word2Vec(sentences=output_sentence_test, vector_size=100, window=5, min_count=1, sg=0)
vector_model_valid.save("word2vec_valid.model")
vector_model_test.save("word2vec_test.model")
model_valid = Word2Vec.load("word2vec_valid.model")
model_test = Word2Vec.load("word2vec_test.model")

len(output_sentence_valid)

# Get word vectors for each word list or sentence
word_vectors_list_valid = []

for word_list in output_sentence_valid:
    word_vectors = [model_valid.wv[word] for word in word_list]
    word_vectors_list_valid.append(word_vectors)

len(word_vectors_list_valid)

output_sentence_valid[8]

word_vectors_list_valid[8]

# Get word vectors for each word list or sentence
word_vectors_list_test = []

for word_list in output_sentence_test:
    word_vectors = [model_test.wv[word] for word in word_list]
    word_vectors_list_test.append(word_vectors)

sentences_test_data[3]

word_vectors_list_test[3]

# numerical_valid_data=pd.read_csv('/content/drive/MyDrive/split_data/numerical_valid_data.csv')
# numerical_valid_data

len(word_vectors_list_valid)

len(word_vectors_list_test)

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors_valid = pad_sequences(word_vectors_list_valid, maxlen=30, padding='pre', dtype='float32')

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences

padded_vectors_test = pad_sequences(word_vectors_list_test, maxlen=30, padding='pre', dtype='float32')



padded_vectors_valid.shape

padded_vectors.shape

padded_vectors_test.shape

# numerical_training_data

# X_train=sentence_vectors
# X_val=sentence_vectors_valid
# X_test=sentence_vectors_test
# y_train=segmented_data['label_encoded']
# y_val=numerical_valid_data['label_encoded']
# y_test=numerical_testing_data['label_encoded']

segmented_data

# from sklearn.svm import SVC
# from sklearn.metrics import accuracy_score

# # Assuming you have X_train, X_val, X_test as input features and y_train, y_val, y_test as output labels

# # Create an SVM classifier
# svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.

# # Train the classifier
# svm_classifier.fit(X_train, y_train)

# # Predict on validation set
# y_val_pred = svm_classifier.predict(X_val)

# # Calculate accuracy on validation set
# val_accuracy = accuracy_score(y_val, y_val_pred)
# print(f"Validation set accuracy: {val_accuracy:.2f}")

# # Predict on test set
# y_test_pred = svm_classifier.predict(X_test)

# # Calculate accuracy on test set
# test_accuracy = accuracy_score(y_test, y_test_pred)
# print(f"Test set accuracy: {test_accuracy:.2f}")

# from sklearn.svm import SVC
# from sklearn.metrics import accuracy_score

# # Assuming you have X_train, X_val, X_test as input features and y_train, y_val, y_test as output labels

# # Create an SVM classifier
# svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.

# # Train the classifier
# svm_classifier.fit(X_train, y_train)

# # Predict on validation set
# y_val_pred = svm_classifier.predict(X_val)

# # Calculate accuracy on validation set
# val_accuracy = accuracy_score(y_val, y_val_pred)
# print(f"Validation set accuracy: {val_accuracy:.2f}")

# # Predict on test set
# y_test_pred = svm_classifier.predict(X_test)

# # Calculate accuracy on test set
# test_accuracy = accuracy_score(y_test, y_test_pred)
# print(f"Test set accuracy: {test_accuracy:.2f}")

# # Calculate overall accuracy
# overall_accuracy = (val_accuracy + test_accuracy) / 2
# print(f"Overall accuracy: {overall_accuracy:.2f}")

# # Import necessary libraries
# from sklearn import svm
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# from sklearn.preprocessing import StandardScaler

# # Assuming you have X_train, y_train, X_valid, y_valid, X_test, y_test

# X_train=sentence_vectors
# X_val=sentence_vectors_valid
# X_test=sentence_vectors_test
# y_train=segmented_data['label_encoded']
# y_val=numerical_valid_data['label_encoded']
# y_test=numerical_testing_data['label_encoded']


# # Preprocess the data (scaling, etc.)
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_valid_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# # Create an SVM model
# svm_model = svm.SVC(kernel='linear')  # You can use other kernels like 'rbf', 'poly', etc.

# # Train the SVM model
# svm_model.fit(X_train_scaled, y_train)

# # Make predictions on the validation set
# y_valid_pred = svm_model.predict(X_valid_scaled)

# # Evaluate the accuracy on the validation set
# accuracy_valid = accuracy_score(y_val, y_valid_pred)
# print(f"Validation Accuracy: {accuracy_valid * 100:.2f}%")

# # Make predictions on the test set
# y_test_pred = svm_model.predict(X_test_scaled)

# # Evaluate the accuracy on the test set
# accuracy_test = accuracy_score(y_test, y_test_pred)
# print(f"Test Accuracy: {accuracy_test * 100:.2f}%")

sentences[4]

word_vectors_list[4]

type(word_vectors_list)

from sklearn.preprocessing import LabelEncoder

# Assuming your original labels are in a column named 'label' in your DataFrame
# Replace 'label' with your actual column name
label_encoder = LabelEncoder()
segmented_data['label_encoded_emotion'] = label_encoder.fit_transform(segmented_data['sentiment'])
numerical_testing_data['label_encoded_emotion'] = label_encoder.fit_transform(numerical_testing_data['sentiment'])
numerical_valid_data['label_encoded_emotion'] = label_encoder.fit_transform(numerical_valid_data['sentiment'])
# Mapping between original labels and encoded labels
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print(label_mapping)

# X_train=np.array(word_vectors_list)
# X_val=np.array(word_vectors_list_valid)
# X_test=np.array(word_vectors_list_test)
y_train=np.array(segmented_data['label_encoded'])
y_val=np.array(numerical_valid_data['label_encoded'])
y_test=np.array(numerical_testing_data['label_encoded'])
y_train_emotion=np.array(segmented_data['label_encoded_emotion'])
y_val_emotion=np.array(numerical_valid_data['label_encoded_emotion'])
y_test_emotion=np.array(numerical_testing_data['label_encoded_emotion'])

y_test.shape

type(y_train)

type(y_test)

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Assuming sentence_vectors_valid is your list of sentence vectors
# Ensure that all sentence vectors have the same dimension (e.g., 100)

# # Convert the list of sentence vectors to a numpy array
# import numpy as np
# sentence_vectors_train = np.array(sentence_vectors)
# sentence_vectors_valid_array = np.array(sentence_vectors_valid)
# sentence_vectors_test_array = np.array(sentence_vectors_test)

sentences_test_data[1792]

# padded_vectors_valid.shape
padded_vectors.shape
# padded_vectors_test.shape

# Reshape the data to be 3-dimensional, with dimensions (number_of_sequences, sequence_length, feature_dimension)
word_vectors_train = padded_vectors
word_vectors_valid = padded_vectors_valid
word_vectors_test = padded_vectors_test

word_vectors_test.shape

print("Number of samples in y_train:", len(y_train))
print("Number of samples in y_val:", len(y_train))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense,Dropout

# Assuming 3 classes (0, 1, 2)
num_classes = 3

model = Sequential()
model.add(LSTM(units=100, input_shape=(30, 100), return_sequences=False))
model.add(Dropout(0.5))
model.add(Dense(units=num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

print("Size of word_vectors_train:", len(word_vectors_train))
print("Size of y_train:", len(y_train))
print("Size of word_vectors_valid:", len(word_vectors_valid))
print("Size of y_val:", len(y_val))

y_val_emotion.shape

# Train the model
history=model.fit(word_vectors_train, y_train,
          epochs=100, batch_size=32,
          validation_data=(word_vectors_valid, y_val))

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))
# Train the model
history_1 = model.fit(word_vectors_train, y_train,
                    epochs=100, batch_size=32,
                    validation_data=(word_vectors_valid, y_val),
                    class_weight=class_weight_dict)  # Assuming you computed class weights

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(word_vectors_test, y_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

history

model.save('your_model_name.h5')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
# Predictions on training set
y_train_pred = np.argmax(model.predict(word_vectors_train), axis=1)

# Predictions on validation set
y_test_pred = np.argmax(model.predict(word_vectors_test), axis=1)

# Confusion matrix for training set
conf_mat_train = confusion_matrix(y_train, y_train_pred)

# Confusion matrix for validation set
conf_mat_test = confusion_matrix(y_test, y_test_pred)

# Class labels
class_labels = ['Negative', 'Neutral', 'Positive']

# Plot confusion matrix for training set
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat_train, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Training Set')
plt.show()

# Plot confusion matrix for validation set
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat_test, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Test Set')
plt.show()

from sklearn.metrics import confusion_matrix
import numpy as np

# Predictions on training set
y_train_pred = np.argmax(model.predict(word_vectors_train), axis=1)

# Predictions on validation set
y_test_pred = np.argmax(model.predict(word_vectors_test), axis=1)

# Confusion matrix for training set
conf_mat_train = confusion_matrix(y_train, y_train_pred)

# Confusion matrix for validation set
conf_mat_test = confusion_matrix(y_test, y_test_pred)

# Class labels
class_labels = ['Negative', 'Neutral', 'Positive']

# Print confusion matrix for training set
print("Confusion Matrix - Training Set:")
print(conf_mat_train)

# Print confusion matrix for validation set
print("\nConfusion Matrix - test Set:")
print(conf_mat_test)

import numpy as np

# Confusion matrix
confusion_matrix = np.array([[151, 307, 155],
                             [130, 313, 154],
                             [133, 311, 139]])

# Calculate precision and recall for each class
precision = np.zeros(3)
recall = np.zeros(3)

for i in range(3):
    true_positive = confusion_matrix[i, i]
    false_positive = np.sum(confusion_matrix[:, i]) - true_positive
    false_negative = np.sum(confusion_matrix[i, :]) - true_positive

    precision[i] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0
    recall[i] = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0

# Print precision and recall for each class
for i in range(3):
    print(f'Class {i + 1}:')
    print(f'Precision: {precision[i]:.4f}')
    print(f'Recall: {recall[i]:.4f}\n')